{"paragraphs":[{"title":"Install - 환경","text":"%md\n## homebrew 설치\n####  간편 설치- mac package관리자  [공식](https://brew.sh/index_ko.html), [간편사용법](https://xho95.github.io/macos/sierra/package/homebrew/issues/2017/01/13/Using-Homebrew-and-some-Issues.html)\n<pre><code>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nbrew update</code></pre>\n\nbrew install apache-spark\nbrew search apache-spark\n\n#### 환경변수 등록 (/etc/profile)\n##### 보통 .bash_profile 에 설정하나 전역적으로 적용하기 위해 /etc/profile에 등록\n##### 나중에 설치할것까지 모두 미리 등록\n<pre>\n<code>\nexport PATH=$(brew --prefix ruby)/bin:$PATH\nexport JAVA_HOME=$(/usr/libexec/java_home -v1.8)\nexport JRE_HOME=$JAVA_HOME/jre\nexport SPARK_HOME=$(brew --prefix apache-spark)\nexport PYTHONPATH=$SPARK_HOME/libexec/python/:$PYTHONP$\nexport SCALA_HOME=$(brew --prefix scala)\nexport MAVEN_HOME=$(brew --prefix maven)\nexport HADOOP_HOME=$(brew --prefix hadoop)\n</code>\n</pre>\n###### 1.9부터는 jre가 없음\n###### /etc/profile를 수정하기 위해서는 root로 로그인하거나 sudo 해야 하며 root계정을 활성화 해야함 [링크](http://wwhite103.tistory.com/71)\n\n#### homebrew - cask 설치 [링크](https://caskroom.github.io/)\n##### brew tap caskroom/cask\n\n## java 설치\n#### oracle사이트 가서 설치 하거나 brew로 설치- 전 다운로드 설치함\n##### brew cask install java\n\n","user":"anonymous","dateUpdated":"2017-11-30T13:01:53+0900","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>homebrew 설치</h2>\n<h4>간편 설치- mac package관리자 <a href=\"https://brew.sh/index_ko.html\">공식</a>, <a href=\"https://xho95.github.io/macos/sierra/package/homebrew/issues/2017/01/13/Using-Homebrew-and-some-Issues.html\">간편사용법</a></h4>\n<pre><code>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nbrew update</code></pre>\n<p>brew install apache-spark<br/>brew search apache-spark</p>\n<h4>환경변수 등록 (/etc/profile)</h4>\n<h5>보통 .bash_profile 에 설정하나 전역적으로 적용하기 위해 /etc/profile에 등록</h5>\n<h5>나중에 설치할것까지 모두 미리 등록</h5>\n<pre>\n<code>\nexport PATH=$(brew --prefix ruby)/bin:$PATH\nexport JAVA_HOME=$(/usr/libexec/java_home -v1.8)\nexport JRE_HOME=$JAVA_HOME/jre\nexport SPARK_HOME=$(brew --prefix apache-spark)\nexport PYTHONPATH=$SPARK_HOME/libexec/python/:$PYTHONP$\nexport SCALA_HOME=$(brew --prefix scala)\nexport MAVEN_HOME=$(brew --prefix maven)\nexport HADOOP_HOME=$(brew --prefix hadoop)\n</code>\n</pre>\n<h6>1.9부터는 jre가 없음</h6>\n<h6>/etc/profile를 수정하기 위해서는 root로 로그인하거나 sudo 해야 하며 root계정을 활성화 해야함 <a href=\"http://wwhite103.tistory.com/71\">링크</a></h6>\n<h4>homebrew - cask 설치 <a href=\"https://caskroom.github.io/\">링크</a></h4>\n<h5>brew tap caskroom/cask</h5>\n<h2>java 설치</h2>\n<h4>oracle사이트 가서 설치 하거나 brew로 설치- 전 다운로드 설치함</h4>\n<h5>brew cask install java</h5>\n</div>"}]},"apps":[],"jobName":"paragraph_1511770783844_1922093130","id":"20171127-171943_1101933678","dateCreated":"2017-11-27T17:19:43+0900","dateStarted":"2017-11-30T13:01:53+0900","dateFinished":"2017-11-30T13:01:55+0900","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1150"},{"title":"install - package","text":"%md\n<pre><code>\nbrew install scala\nbrew install maven\nbrew install apache-spark\nbrew install hadoop\nbrew install apache-zeppelin\nbrew install openssl\n</code></pre>\n** 실환경은 linux로 구축하여야함\n** hadoop spark 는 linux 계정을 생성 후 설치 및 ssh연동 작업을 진행하여아함\n** 파일 시스템은 꼭 hadoop일 필요는 없음(Hadoop Distributed File System (HDFS), MapR File System (MapR-FS), Cassandra, OpenStack Swift, Amazon S3, Kudu, glusterfs, chif)\n** Cluster Manager 는 Hadoop YARN, or Apache Mesos 연동 (http://www.mimul.com/pebble/default/2013/10/27/1382885361083.html)\n** hadoop설정 링크 (http://hajiz.tistory.com/entry/spark%EB%A5%BC-%EC%9C%84%ED%95%9C-hadoop-%EC%84%A4%EC%A0%95)","user":"anonymous","dateUpdated":"2017-11-28T18:18:21+0900","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<pre><code>\nbrew install scala\nbrew install maven\nbrew install apache-spark\nbrew install hadoop\nbrew install apache-zeppelin\nbrew install openssl\n</code></pre>\n<p>** 실환경은 linux로 구축하여야함<br/>** hadoop spark 는 linux 계정을 생성 후 설치 및 ssh연동 작업을 진행하여아함<br/>** 파일 시스템은 꼭 hadoop일 필요는 없음(Hadoop Distributed File System (HDFS), MapR File System (MapR-FS), Cassandra, OpenStack Swift, Amazon S3, Kudu, glusterfs, chif)<br/>** Cluster Manager 는 Hadoop YARN, or Apache Mesos 연동 (<a href=\"http://www.mimul.com/pebble/default/2013/10/27/1382885361083.html\">http://www.mimul.com/pebble/default/2013/10/27/1382885361083.html</a>)<br/>** hadoop설정 링크 (<a href=\"http://hajiz.tistory.com/entry/spark%EB%A5%BC-%EC%9C%84%ED%95%9C-hadoop-%EC%84%A4%EC%A0%95\">http://hajiz.tistory.com/entry/spark%EB%A5%BC-%EC%9C%84%ED%95%9C-hadoop-%EC%84%A4%EC%A0%95</a>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511831593826_-1586483553","id":"20171128-101313_1818382246","dateCreated":"2017-11-28T10:13:13+0900","dateStarted":"2017-11-28T18:18:21+0900","dateFinished":"2017-11-28T18:18:21+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1151"},{"text":"%spark\nspark.version\n","user":"anonymous","dateUpdated":"2017-11-30T13:05:50+0900","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":"/usr/local/opt/apache-spark/bin/load-spark-env.sh: line 2: /usr/local/Cellar/apache-spark/2.2.0/libexec/bin/load-spark-env.sh: Permission denied\n/usr/local/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /usr/local/Cellar/apache-spark/2.2.0/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n","apps":[],"jobName":"paragraph_1511862737930_58149134","id":"20171128-185217_2133683619","dateCreated":"2017-11-28T18:52:17+0900","dateStarted":"2017-11-30T13:05:50+0900","dateFinished":"2017-11-30T13:05:51+0900","status":"ERROR","errorMessage":"org.apache.zeppelin.interpreter.InterpreterException: /usr/local/opt/apache-spark/bin/load-spark-env.sh: line 2: /usr/local/Cellar/apache-spark/2.2.0/libexec/bin/load-spark-env.sh: Permission denied\n/usr/local/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /usr/local/Cellar/apache-spark/2.2.0/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:143)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.reference(RemoteInterpreterProcess.java:73)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:265)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:430)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.getFormType(LazyOpenInterpreter.java:111)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:387)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:329)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n","progressUpdateIntervalMs":500,"$$hashKey":"object:1152"},{"text":"%md\n이클립스에 scala ide 설치하기\n\neclipse 에서 help->eclipse market place 선택->“scala”로 검색->The Scala IDE for Eclipse 의 Install 버튼 클릭->Next->I accept~ 체크->Finish->설치완료 후 eclipse 재시작\nInstall m2eclipse-scala\n\nHelp->Install new software-> work with 에 http://alchim31.free.fr/m2e-scala/update-site 입력 후 엔터->설치-> eclipse 재시작\n\n(참고:http://scala-ide.org/docs/tutorials/m2eclipse/index.html)","user":"anonymous","dateUpdated":"2017-11-29T15:52:21+0900","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>이클립스에 scala ide 설치하기</p>\n<p>eclipse 에서 help-&gt;eclipse market place 선택-&gt;“scala”로 검색-&gt;The Scala IDE for Eclipse 의 Install 버튼 클릭-&gt;Next-&gt;I accept~ 체크-&gt;Finish-&gt;설치완료 후 eclipse 재시작<br/>Install m2eclipse-scala</p>\n<p>Help-&gt;Install new software-&gt; work with 에 <a href=\"http://alchim31.free.fr/m2e-scala/update-site\">http://alchim31.free.fr/m2e-scala/update-site</a> 입력 후 엔터-&gt;설치-&gt; eclipse 재시작</p>\n<p>(참고:<a href=\"http://scala-ide.org/docs/tutorials/m2eclipse/index.html\">http://scala-ide.org/docs/tutorials/m2eclipse/index.html</a>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511938332053_1770927384","id":"20171129-155212_369175382","dateCreated":"2017-11-29T15:52:12+0900","dateStarted":"2017-11-29T15:52:21+0900","dateFinished":"2017-11-29T15:52:21+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1153"},{"text":"import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\n// 1.4.1절\nobject WordCount {\n\n  def main(args: Array[String]): Unit = {\n\n    require(args.length == 3, \"Usage: WordCount <Master> <Input> <Output>\")\n\n    val sc = getSparkContext(\"WordCount\", args(0))\n\n    val inputRDD = getInputRDD(sc, args(1))\n\n    val resultRDD = process(inputRDD)\n\n    handleResult(resultRDD, args(2))\n  }\n\n  def getSparkContext(appName: String, master: String) = {\n    val conf = new SparkConf().setAppName(appName).setMaster(master)\n    new SparkContext(conf)\n  }\n\n  def getInputRDD(sc: SparkContext, input: String) = {\n    sc.textFile(input)\n  }\n\n  def process(inputRDD: RDD[String]) = {\n    val words = inputRDD.flatMap(str => str.split(\" \"))\n    val wcPair = words.map((_, 1))\n    wcPair.reduceByKey(_ + _)\n  }\n\n  def handleResult(resultRDD: RDD[(String, Int)], output: String) {\n    resultRDD.saveAsTextFile(output);\n  }\n}\n","user":"anonymous","dateUpdated":"2017-11-28T18:28:30+0900","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\ndefined object WordCount\n"}]},"apps":[],"jobName":"paragraph_1511861198197_1938359893","id":"20171128-182638_560892633","dateCreated":"2017-11-28T18:26:38+0900","dateStarted":"2017-11-28T18:28:31+0900","dateFinished":"2017-11-28T18:28:34+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1154"},{"text":"var inputRDD = sc.textFile(\"/Users/bootcode/jxbrowser-browser.log\");\nval words = inputRDD.flatMap(str => str.split(\" \"))\nval wcPair = words.map((_, 1))\nvar resultRDD = wcPair.reduceByKey(_ + _);\nresultRDD.foreach( println )\n//resultRDD.saveAsTextFile(\"/Users/bootcode/result.txt\");\n\n","user":"anonymous","dateUpdated":"2017-11-28T19:02:55+0900","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","lineNumbers":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"inputRDD: org.apache.spark.rdd.RDD[String] = /Users/bootcode/jxbrowser-browser.log MapPartitionsRDD[43] at textFile at <console>:29\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[44] at flatMap at <console>:31\nwcPair: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[45] at map at <console>:33\nresultRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[46] at reduceByKey at <console>:35\n"}]},"apps":[],"jobName":"paragraph_1511861396348_-608602849","id":"20171128-182956_613730576","dateCreated":"2017-11-28T18:29:56+0900","dateStarted":"2017-11-28T19:02:45+0900","dateFinished":"2017-11-28T19:02:48+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1155"},{"text":"spark.catalog.listTables.show","user":"anonymous","dateUpdated":"2017-11-28T19:05:33+0900","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------+-----------+---------+-----------+\n|name|database|description|tableType|isTemporary|\n+----+--------+-----------+---------+-----------+\n+----+--------+-----------+---------+-----------+\n\n"}]},"apps":[],"jobName":"paragraph_1511861499558_268963419","id":"20171128-183139_583490589","dateCreated":"2017-11-28T18:31:39+0900","dateStarted":"2017-11-28T19:05:33+0900","dateFinished":"2017-11-28T19:05:37+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1156"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511863533745_-2088603588","id":"20171128-190533_991268325","dateCreated":"2017-11-28T19:05:33+0900","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1157"}],"name":"Spark-mac","id":"2CZXY4NYV","angularObjects":{"2D1EF7UFN:shared_process":[],"2CYPMB4NA:shared_process":[],"2CYP4M1W7:shared_process":[],"2CZXD894K:shared_process":[],"2CY5QM175:shared_process":[],"2D2HUKSWV:shared_process":[],"2D27QRC9V:shared_process":[],"2D1913QET:shared_process":[],"2CZ3TR5UJ:shared_process":[],"2D2GXTAEE:shared_process":[],"2D2JJ27BM:shared_process":[],"2CZMXEVAJ:shared_process":[],"2CYJYW6U3:shared_process":[],"2D1KQWCFU:shared_process":[],"2CXS2D1GA:shared_process":[],"2D1KFC3GY:shared_process":[],"2D1XFQ784:shared_process":[],"2CY61VS6E:shared_process":[],"2CYU13Z95:shared_process":[],"2D216JVSC:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}